{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_5_FCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rugeYYiqsrlc"
   },
   "source": [
    "This file is based on the tutorial based on:\n",
    "\n",
    "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QXZ6Tuqc9Q-l"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lI0GbgLgpkos",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2fhRixcspkot",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /tmp/data/mnist\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Grayscale(num_output_channels=1)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0,), std=(1,))\n",
      "           )\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: /tmp/data/mnist\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Grayscale(num_output_channels=1)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0,), std=(1,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train)\n",
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aEtCbO6upkou",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhFyzySNeT_e"
   },
   "source": [
    "# Define the Network(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN_Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-uquHLLmpkox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# accepts a tensor of batchx784\n",
    "# static coding used - the same value passed every time\n",
    "class SNN_Baseline(nn.Module):\n",
    "    def __init__(self, num_steps=25, beta=0.95, num_hidden=1000):\n",
    "        super().__init__()\n",
    "        self.name = \"SNN_Baseline\"\n",
    "        \n",
    "        # Temporal Dynamics\n",
    "        self.num_steps = num_steps \n",
    "        self.beta = beta\n",
    "        \n",
    "        # Network Architecture\n",
    "        num_inputs = 28*28\n",
    "        num_outputs = 10\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN_Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts a tensor of batchx784\n",
    "# static coding used - the same value passed every time\n",
    "class SNN_Large(nn.Module):\n",
    "    def __init__(self, num_steps=25, beta=0.95, num_hidden=1000):\n",
    "        super().__init__()\n",
    "        self.name = \"SNN_Large\"\n",
    "        \n",
    "        # Temporal Dynamics\n",
    "        self.num_steps = num_steps \n",
    "        self.beta = beta\n",
    "        \n",
    "        # Network Architecture\n",
    "        num_inputs = 28*28\n",
    "        num_outputs = 10\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta)\n",
    "        self.fc3 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spkx_rec = []\n",
    "        memx_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spkx_rec.append(spk3)\n",
    "            memx_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spkx_rec, dim=0), torch.stack(memx_rec, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN_Clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts a tensor of batchx784\n",
    "# static coding used - the same value passed every time\n",
    "class SNN_Clamp(nn.Module):\n",
    "    def __init__(self, num_steps=25, beta=0.95, num_hidden=1000):\n",
    "        super().__init__()\n",
    "        self.name = \"SNN_Clamp\"\n",
    "        \n",
    "        # Temporal Dynamics\n",
    "        self.num_steps = num_steps \n",
    "        self.beta = beta\n",
    "        \n",
    "        # Network Architecture\n",
    "        num_inputs = 28*28\n",
    "        num_outputs = 10\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, 500)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta)\n",
    "        self.fc3 = nn.Linear(500, num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spkx_rec = []\n",
    "        memx_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spkx_rec.append(spk3)\n",
    "            memx_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spkx_rec, dim=0), torch.stack(memx_rec, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a7MdORCtIx4"
   },
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-IxcnBAxpkoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist):\n",
    "    fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "    plt.plot(loss_hist)\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # drop_last switched to False to keep all samples\n",
    "    testLoader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      net.eval()\n",
    "      for data, targets in testLoader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        test_spk, _ = net(data.view(data.size(0), -1))\n",
    "    \n",
    "        # calculate total accuracy\n",
    "        _, predicted = test_spk.sum(dim=0).max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "    print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, num_epochs):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "    \n",
    "    loss_hist = []\n",
    "    counter = 0\n",
    "    \n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_counter = 0\n",
    "        train_batch = iter(train_loader)\n",
    "    \n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "    \n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(net.num_steps):\n",
    "                loss_val += loss(mem_rec[step], targets)\n",
    "    \n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            '''\n",
    "            # Test set\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                test_data, test_targets = next(iter(test_loader))\n",
    "                test_data = test_data.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "    \n",
    "                # Test set forward pass\n",
    "                test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "    \n",
    "                # Test set loss\n",
    "                test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "                for step in range(net.num_steps):\n",
    "                    test_loss += loss(test_mem[step], test_targets)\n",
    "                test_loss_hist.append(test_loss.item())\n",
    "    \n",
    "                # Print train/test loss/accuracy\n",
    "                if counter % 10000 == 0:\n",
    "                    train_printer(\n",
    "                        data, targets, epoch,\n",
    "                        counter, iter_counter,\n",
    "                        loss_hist, test_loss_hist,\n",
    "                        test_data, test_targets)\n",
    "            '''\n",
    "            counter += 1\n",
    "            iter_counter +=1\n",
    "\n",
    "            print(\"\\rProgress %5d\\t%8d / %8d\" % (counter, counter*batch_size, len(mnist_train)*num_epochs), end='', flush=True)\n",
    "    return loss_hist\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples of format (model, num_steps, beta, num_hidden, num_epochs)\n",
    "hyperList = [\n",
    "    (SNN_Baseline, 5, 0.95, 3000, 2),\n",
    "    (SNN_Baseline, 10, 0.95, 3000, 2),\n",
    "    (SNN_Baseline, 10, 0.95, 3000, 30),\n",
    "    (SNN_Baseline, 10, 0.95, 3000, 90),\n",
    "    (SNN_Baseline, 15, 0.95, 3000, 2),\n",
    "    (SNN_Baseline, 20, 0.95, 3000, 2),\n",
    "    (SNN_Baseline, 25, 0.95, 3000, 30),\n",
    "    (SNN_Baseline, 30, 0.95, 3000, 2),\n",
    "    (SNN_Baseline, 40, 0.95, 3000, 2),\n",
    "    (SNN_Large, 40, 0.95, 3000, 2),\n",
    "    (SNN_Clamp, 40, 0.95, 5000, 30),\n",
    "    (SNN_Baseline, 40, 0.95, 5000, 30),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache\n",
    "csv_file = 'hyper_result_cache.csv'\n",
    "results = {}\n",
    "if os.path.exists(csv_file):\n",
    "    with open(csv_file, mode='r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            key_str, result = row\n",
    "            results[key_str] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached hyper (<class '__main__.SNN_Baseline'>, 5, 0.95, 3000, 2)\n",
      "Test Set Accuracy: 93.62%\n",
      "Cached hyper (<class '__main__.SNN_Baseline'>, 10, 0.95, 3000, 2)\n",
      "Test Set Accuracy: 95.02%\n",
      "Cached hyper (<class '__main__.SNN_Baseline'>, 10, 0.95, 3000, 30)\n",
      "Test Set Accuracy: 95.22%\n",
      "Training for (<class '__main__.SNN_Baseline'>, 10, 0.95, 3000, 90)\n",
      "Progress 24296\t 3109888 /  5400000"
     ]
    }
   ],
   "source": [
    "for hyper in hyperList:\n",
    "\n",
    "    shyper = str(hyper)\n",
    "    if (shyper in results) and True:\n",
    "        print(\"Cached hyper\", hyper)\n",
    "        print(f\"Test Set Accuracy: {100 * float(results[shyper]):.2f}%\")\n",
    "        continue\n",
    "        \n",
    "    print(\"Training for\", hyper)\n",
    "    startTime = time.time()\n",
    "    \n",
    "    net = hyper[0](hyper[1], hyper[2], hyper[3])\n",
    "    loss_hist = train_net(net = net.to(device),\n",
    "                          num_epochs = hyper[4])\n",
    "\n",
    "    print(\"\\nFinished, time elapsed %.2f s\" % (time.time()-startTime))\n",
    "    plot_loss(loss_hist)\n",
    "    accu = evaluate(net)\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "    with open(csv_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        results[shyper] = accu\n",
    "        writer.writerow([shyper, accu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"(<class '__main__.SNN_Baseline'>, 25, 0.95, 10000, 1)\": '0.9464',\n",
       " \"(<class '__main__.SNN_Baseline'>, 25, 0.95, 3000, 1)\": '0.938',\n",
       " \"(<class '__main__.SNN_Large'>, 25, 0.95, 3000, 1)\": '0.9033',\n",
       " \"(<class '__main__.SNN_Baseline'>, 25, 0.95, 3000, 2)\": '0.9656',\n",
       " \"(<class '__main__.SNN_Baseline'>, 25, 0.95, 10000, 2)\": '0.9576',\n",
       " \"(<class '__main__.SNN_Large'>, 25, 0.95, 3000, 2)\": '0.8959',\n",
       " \"(<class '__main__.SNN_Baseline'>, 5, 0.95, 3000, 2)\": '0.9362',\n",
       " \"(<class '__main__.SNN_Baseline'>, 10, 0.95, 3000, 2)\": '0.9502',\n",
       " \"(<class '__main__.SNN_Baseline'>, 15, 0.95, 3000, 2)\": '0.959',\n",
       " \"(<class '__main__.SNN_Baseline'>, 20, 0.95, 3000, 2)\": '0.9523',\n",
       " \"(<class '__main__.SNN_Baseline'>, 30, 0.95, 3000, 2)\": '0.9583',\n",
       " \"(<class '__main__.SNN_Baseline'>, 40, 0.95, 1000, 1)\": '0.9215',\n",
       " \"(<class '__main__.SNN_Baseline'>, 40, 0.95, 1000, 2)\": '0.9478',\n",
       " \"(<class '__main__.SNN_Baseline'>, 40, 0.95, 3000, 2)\": '0.9423',\n",
       " \"(<class '__main__.SNN_Large'>, 40, 0.95, 3000, 2)\": '0.9081',\n",
       " \"(<class '__main__.SNN_Clamp'>, 40, 0.95, 3000, 2)\": '0.8842',\n",
       " \"(<class '__main__.SNN_Clamp'>, 40, 0.95, 5000, 2)\": '0.9067',\n",
       " \"(<class '__main__.SNN_Baseline'>, 25, 0.95, 3000, 10)\": 0.9659}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "snntorch_tutorial_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
